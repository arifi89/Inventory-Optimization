{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230a1eb9",
   "metadata": {},
   "source": [
    "# 02 - Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1834e00-a632-4905-a58f-da679117894c",
   "metadata": {},
   "source": [
    "\n",
    "***INVENTORY OPTIMIZATION WITH PROCUREMENT STRATEGY***\n",
    "================================================================================\n",
    "# Notebook 02: Data Cleaning & Preprocessing\n",
    "## Author: Mohamed Osman\n",
    "## Date: January 2025\n",
    "## GitHub: https://github.com/arifi89\n",
    "## LinkedIn: https://www.linkedin.com/in/mohamed-osman-123456789/\n",
    "\n",
    "================================================================================\n",
    "## ***OBJECTIVE:***\n",
    "This notebook performs comprehensive data cleaning and preprocessing to prepare\n",
    "our datasets for analysis. We will:\n",
    "\n",
    " **1. Handle missing values across all datasets**.\n",
    " **2. Standardize column names and data types**.\n",
    " **3. Parse and validate dates**.\n",
    " **4. Remove duplicates and outliers**.\n",
    " **5. Create unique product identifiers**.\n",
    " **6. Validate data integrity**.\n",
    " **7. Export cleaned datasets to data/processed/**.\n",
    "\n",
    "## ***KEY FINDINGS FROM NOTEBOOK 01:***\n",
    "##### ‚Ä¢ Invoice Purchases has 9.33% missing values (5,169 cells)\n",
    "##### ‚Ä¢ Ending Inventory has 0.06% missing values (1,284 cells)\n",
    "##### ‚Ä¢ Purchases has 3 missing values (0.00%)\n",
    "##### ‚Ä¢ Future Prices has 3 missing values (0.00%)\n",
    "##### ‚Ä¢ The Approval column in Invoice Purchases needs attention\n",
    "\n",
    "## ***DATA CLEANING STRATEGY:***\n",
    "##### 1. Remove unnecessary columns (e.g., Approval column with high missing values)\n",
    "##### 2. Standardize column names (lowercase, underscores)\n",
    "##### 3. Convert date columns to datetime format\n",
    "##### 4. Handle missing values appropriately for each dataset\n",
    "##### 5. Validate numeric columns and identify outliers\n",
    "##### 6. Create a master dataset for analysis\n",
    "\n",
    "================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ef92e0",
   "metadata": {},
   "source": [
    "## SECTION 1: ENVIRONMENT SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bef397cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üì¶ SECTION 1: ENVIRONMENT SETUP\n",
      "================================================================================\n",
      "‚úÖ Libraries imported successfully\n",
      "‚úÖ Display options configured\n",
      "‚úÖ Visualization style set\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üì¶ SECTION 1: ENVIRONMENT SETUP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(\"‚úÖ Display options configured\")\n",
    "print(\"‚úÖ Visualization style set\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73bbf8d",
   "metadata": {},
   "source": [
    "## SECTION 2: LOAD RAW DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d09e850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìÇ SECTION 2: LOAD RAW DATA\n",
      "================================================================================\n",
      "‚úÖ Processed data directory ready: ..\\data\\processed\n",
      "\n",
      "‚è≥ Loading datasets...\n",
      "\n",
      "‚úÖ beginning_inventory: 206,529 rows √ó 9 columns\n",
      "‚úÖ purchases: 2,372,474 rows √ó 16 columns\n",
      "‚úÖ invoice_purchases: 5,543 rows √ó 10 columns\n",
      "‚úÖ sales: 1,048,575 rows √ó 14 columns\n",
      "‚úÖ ending_inventory: 224,489 rows √ó 9 columns\n",
      "‚úÖ future_prices: 12,261 rows √ó 9 columns\n",
      "\n",
      "‚úÖ All datasets loaded successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìÇ SECTION 2: LOAD RAW DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define the base path to raw data folder\n",
    "RAW_PATH = Path(\"../data/raw\")\n",
    "PROCESSED_PATH = Path(\"../data/processed\")\n",
    "\n",
    "# Create processed directory if it doesn't exist\n",
    "PROCESSED_PATH.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"‚úÖ Processed data directory ready: {PROCESSED_PATH}\\n\")\n",
    "\n",
    "# Define all file paths\n",
    "file_paths = {\n",
    "    'beginning_inventory': RAW_PATH / \"BegInvFINAL12312016.csv\",\n",
    "    'purchases': RAW_PATH / \"PurchasesFINAL12312016.csv\",\n",
    "    'invoice_purchases': RAW_PATH / \"InvoicePurchases12312016.csv\",\n",
    "    'sales': RAW_PATH / \"SalesFINAL12312016.csv\",\n",
    "    'ending_inventory': RAW_PATH / \"EndInvFINAL12312016.csv\",\n",
    "    'future_prices': RAW_PATH / \"2017PurchasePricesDec.csv\"\n",
    "}\n",
    "\n",
    "# Load each dataset\n",
    "print(\"‚è≥ Loading datasets...\\n\")\n",
    "dataframes = {}\n",
    "\n",
    "for name, path in file_paths.items():\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        dataframes[name] = df\n",
    "        print(f\"‚úÖ {name}: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {name}: {str(e)}\")\n",
    "\n",
    "# Create individual variables\n",
    "beg_inv = dataframes['beginning_inventory'].copy()\n",
    "purchases = dataframes['purchases'].copy()\n",
    "invoice_purchases = dataframes['invoice_purchases'].copy()\n",
    "sales = dataframes['sales'].copy()\n",
    "end_inv = dataframes['ending_inventory'].copy()\n",
    "future_prices = dataframes['future_prices'].copy()\n",
    "\n",
    "print(f\"\\n‚úÖ All datasets loaded successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebaec5a",
   "metadata": {},
   "source": [
    "## SECTION 3: INITIAL DATA INSPECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd3481fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç SECTION 3: INITIAL DATA INSPECTION\n",
      "================================================================================\n",
      "\n",
      "üìã BEGINNING_INVENTORY\n",
      "--------------------------------------------------------------------------------\n",
      "Shape: (206529, 9)\n",
      "\n",
      "Columns: ['InventoryId', 'Store', 'City', 'Brand', 'Description', 'Size', 'onHand', 'Price', 'startDate']\n",
      "\n",
      "Data Types:\n",
      "InventoryId        str\n",
      "Store            int64\n",
      "City               str\n",
      "Brand            int64\n",
      "Description        str\n",
      "Size               str\n",
      "onHand           int64\n",
      "Price          float64\n",
      "startDate          str\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "Series([], dtype: int64)\n",
      "‚úÖ No missing values\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìã PURCHASES\n",
      "--------------------------------------------------------------------------------\n",
      "Shape: (2372474, 16)\n",
      "\n",
      "Columns: ['InventoryId', 'Store', 'Brand', 'Description', 'Size', 'VendorNumber', 'VendorName', 'PONumber', 'PODate', 'ReceivingDate', 'InvoiceDate', 'PayDate', 'PurchasePrice', 'Quantity', 'Dollars', 'Classification']\n",
      "\n",
      "Data Types:\n",
      "InventoryId           str\n",
      "Store               int64\n",
      "Brand               int64\n",
      "Description           str\n",
      "Size                  str\n",
      "VendorNumber        int64\n",
      "VendorName            str\n",
      "PONumber            int64\n",
      "PODate                str\n",
      "ReceivingDate         str\n",
      "InvoiceDate           str\n",
      "PayDate               str\n",
      "PurchasePrice     float64\n",
      "Quantity            int64\n",
      "Dollars           float64\n",
      "Classification      int64\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "Size    3\n",
      "dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìã INVOICE_PURCHASES\n",
      "--------------------------------------------------------------------------------\n",
      "Shape: (5543, 10)\n",
      "\n",
      "Columns: ['VendorNumber', 'VendorName', 'InvoiceDate', 'PONumber', 'PODate', 'PayDate', 'Quantity', 'Dollars', 'Freight', 'Approval']\n",
      "\n",
      "Data Types:\n",
      "VendorNumber      int64\n",
      "VendorName          str\n",
      "InvoiceDate         str\n",
      "PONumber          int64\n",
      "PODate              str\n",
      "PayDate             str\n",
      "Quantity          int64\n",
      "Dollars         float64\n",
      "Freight         float64\n",
      "Approval            str\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "Approval    5169\n",
      "dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìã SALES\n",
      "--------------------------------------------------------------------------------\n",
      "Shape: (1048575, 14)\n",
      "\n",
      "Columns: ['InventoryId', 'Store', 'Brand', 'Description', 'Size', 'SalesQuantity', 'SalesDollars', 'SalesPrice', 'SalesDate', 'Volume', 'Classification', 'ExciseTax', 'VendorNo', 'VendorName']\n",
      "\n",
      "Data Types:\n",
      "InventoryId           str\n",
      "Store               int64\n",
      "Brand               int64\n",
      "Description           str\n",
      "Size                  str\n",
      "SalesQuantity       int64\n",
      "SalesDollars      float64\n",
      "SalesPrice        float64\n",
      "SalesDate             str\n",
      "Volume              int64\n",
      "Classification      int64\n",
      "ExciseTax         float64\n",
      "VendorNo            int64\n",
      "VendorName            str\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "Series([], dtype: int64)\n",
      "‚úÖ No missing values\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìã ENDING_INVENTORY\n",
      "--------------------------------------------------------------------------------\n",
      "Shape: (224489, 9)\n",
      "\n",
      "Columns: ['InventoryId', 'Store', 'City', 'Brand', 'Description', 'Size', 'onHand', 'Price', 'endDate']\n",
      "\n",
      "Data Types:\n",
      "InventoryId        str\n",
      "Store            int64\n",
      "City               str\n",
      "Brand            int64\n",
      "Description        str\n",
      "Size               str\n",
      "onHand           int64\n",
      "Price          float64\n",
      "endDate            str\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "City    1284\n",
      "dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìã FUTURE_PRICES\n",
      "--------------------------------------------------------------------------------\n",
      "Shape: (12261, 9)\n",
      "\n",
      "Columns: ['Brand', 'Description', 'Price', 'Size', 'Volume', 'Classification', 'PurchasePrice', 'VendorNumber', 'VendorName']\n",
      "\n",
      "Data Types:\n",
      "Brand               int64\n",
      "Description           str\n",
      "Price             float64\n",
      "Size                  str\n",
      "Volume                str\n",
      "Classification      int64\n",
      "PurchasePrice     float64\n",
      "VendorNumber        int64\n",
      "VendorName            str\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "Description    1\n",
      "Size           1\n",
      "Volume         1\n",
      "dtype: int64\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç SECTION 3: INITIAL DATA INSPECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def inspect_dataset(df, name):\n",
    "    \"\"\"\n",
    "    Quick inspection of dataset before cleaning\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìã {name.upper()}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    print(f\"\\nData Types:\\n{df.dtypes}\")\n",
    "    print(f\"\\nMissing Values:\\n{df.isnull().sum()[df.isnull().sum() > 0]}\")\n",
    "    if df.isnull().sum().sum() == 0:\n",
    "        print(\"‚úÖ No missing values\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Inspect each dataset\n",
    "for name, df in dataframes.items():\n",
    "    inspect_dataset(df, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9d9ef2",
   "metadata": {},
   "source": [
    "## SECTION 4: HANDLE MISSING VALUES - INVOICE PURCHASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe8d4194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üßπ SECTION 4: HANDLE MISSING VALUES - INVOICE PURCHASES\n",
      "================================================================================\n",
      "\n",
      "üìä Before Cleaning:\n",
      "   Shape: (5543, 9)\n",
      "   Columns: ['vendornumber', 'vendorname', 'invoicedate', 'ponumber', 'podate', 'paydate', 'quantity', 'dollars', 'freight']\n",
      "\n",
      "   Missing Values by Column:\n",
      "\n",
      "üìù DATA CLEANING DECISION:\n",
      "   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ\n",
      "   COLUMN TO REMOVE: 'Approval'\n",
      "   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ   ‚îÄ\n",
      "   REASON:\n",
      "   ‚ö†Ô∏è  'Approval' column not found in dataset\n",
      "\n",
      "üìä After Cleaning:\n",
      "   Shape: (5543, 9)\n",
      "   Columns: ['vendornumber', 'vendorname', 'invoicedate', 'ponumber', 'podate', 'paydate', 'quantity', 'dollars', 'freight']\n",
      "   Total Missing Values: 0\n",
      "   Data Completeness: 100.00%\n",
      "\n",
      "‚úÖ Invoice Purchases dataset cleaned successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üßπ SECTION 4: HANDLE MISSING VALUES - INVOICE PURCHASES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Before Cleaning:\")\n",
    "print(f\"   Shape: {invoice_purchases.shape}\")\n",
    "print(f\"   Columns: {list(invoice_purchases.columns)}\")\n",
    "print(f\"\\n   Missing Values by Column:\")\n",
    "missing_inv = invoice_purchases.isnull().sum()\n",
    "missing_pct_inv = (invoice_purchases.isnull().sum() / len(invoice_purchases)) * 100\n",
    "\n",
    "for col in invoice_purchases.columns:\n",
    "    if missing_inv[col] > 0:\n",
    "        print(f\"      ‚Ä¢ {col}: {missing_inv[col]:,} ({missing_pct_inv[col]:.2f}%)\")\n",
    "\n",
    "# Document the decision to remove Approval column\n",
    "print(\"\\nüìù DATA CLEANING DECISION:\")\n",
    "print(\"   ‚îÄ\" * 40)\n",
    "print(\"   COLUMN TO REMOVE: 'Approval'\")\n",
    "print(\"   ‚îÄ\" * 40)\n",
    "print(\"   REASON:\")\n",
    "\n",
    "if 'Approval' in invoice_purchases.columns:\n",
    "    approval_missing = invoice_purchases['Approval'].isnull().sum()\n",
    "    approval_pct = (approval_missing / len(invoice_purchases)) * 100\n",
    "    print(f\"   ‚Ä¢ High percentage of missing values: {approval_missing:,} ({approval_pct:.2f}%)\")\n",
    "    print(f\"   ‚Ä¢ Missing values represent {approval_pct:.1f}% of total records\")\n",
    "    print(\"   ‚Ä¢ Approval status is not critical for inventory optimization analysis\")\n",
    "    print(\"   ‚Ä¢ Removing column improves data quality without losing analytical value\")\n",
    "    print(\"\\n   IMPACT:\")\n",
    "    print(\"   ‚Ä¢ This column will be excluded from further analysis\")\n",
    "    print(\"   ‚Ä¢ All other invoice purchase data remains intact\")\n",
    "    print(\"   ‚Ä¢ Data completeness will improve significantly\")\n",
    "    \n",
    "    # Remove the Approval column\n",
    "    invoice_purchases_cleaned = invoice_purchases.drop(columns=['Approval'])\n",
    "    print(\"\\n   ‚úÖ ACTION COMPLETED: 'Approval' column removed\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  'Approval' column not found in dataset\")\n",
    "    invoice_purchases_cleaned = invoice_purchases.copy()\n",
    "\n",
    "print(\"\\nüìä After Cleaning:\")\n",
    "print(f\"   Shape: {invoice_purchases_cleaned.shape}\")\n",
    "print(f\"   Columns: {list(invoice_purchases_cleaned.columns)}\")\n",
    "print(f\"   Total Missing Values: {invoice_purchases_cleaned.isnull().sum().sum()}\")\n",
    "print(f\"   Data Completeness: {((len(invoice_purchases_cleaned) * len(invoice_purchases_cleaned.columns) - invoice_purchases_cleaned.isnull().sum().sum()) / (len(invoice_purchases_cleaned) * len(invoice_purchases_cleaned.columns)) * 100):.2f}%\")\n",
    "\n",
    "# Update the main variable\n",
    "invoice_purchases = invoice_purchases_cleaned\n",
    "\n",
    "print(\"\\n‚úÖ Invoice Purchases dataset cleaned successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1f7b05",
   "metadata": {},
   "source": [
    "## SECTION 5: STANDARDIZE COLUMN NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f770ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üßº SECTION 5: COMPLETE DATA CLEANING & STANDARDIZATION\n",
      "================================================================================\n",
      "\n",
      "üîÑ Cleaning and standardizing all datasets...\n",
      "\n",
      "\n",
      "üìù Beginning Inventory:\n",
      "   Before: ['Inventory_Id', 'Store', 'City', 'Brand', 'Description', 'Size', 'On_Hand', 'Sales_Price', 'Start_Date']\n",
      "   After:  ['Inventory_Id', 'Store', 'City', 'Brand', 'Description', 'Size', 'On_Hand', 'Sales_Price', 'Start_Date']\n",
      "\n",
      "üìù Ending Inventory:\n",
      "   Before: ['Inventory_Id', 'Store', 'City', 'Brand', 'Description', 'Size', 'On_Hand', 'Sales_Price', 'End_Date']\n",
      "   After:  ['Inventory_Id', 'Store', 'City', 'Brand', 'Description', 'Size', 'On_Hand', 'Sales_Price', 'End_Date']\n",
      "\n",
      "üìù Purchases:\n",
      "   Before: ['Po_Date', 'Po_Number', 'Vendor_Number', 'Vendor_Name', 'Store', 'Inventory_Id', 'Brand', 'Description', 'Size', 'Unit_Price', 'Quantity', 'Total_Price', 'Receiving_Date', 'Pay_Date', 'Classification', 'Invoice_Date']\n",
      "   After:  ['Po_Date', 'Po_Number', 'Vendor_Number', 'Vendor_Name', 'Store', 'Inventory_Id', 'Brand', 'Description', 'Size', 'Unit_Price', 'Quantity', 'Total_Price', 'Receiving_Date', 'Pay_Date', 'Classification', 'Invoice_Date']\n",
      "\n",
      "üìù Invoice Purchases:\n",
      "   Before: ['Vendor_Number', 'Vendor_Name', 'Invoice_Date', 'Po_Number', 'Po_Date', 'Pay_Date', 'Quantity', 'Total_Price', 'Freight_Cost']\n",
      "   After:  ['Vendor_Number', 'Vendor_Name', 'Invoice_Date', 'Po_Number', 'Po_Date', 'Pay_Date', 'Quantity', 'Total_Price', 'Freight_Cost']\n",
      "\n",
      "üìù Sales:\n",
      "   Before: ['Sales_Date', 'Store', 'Inventory_Id', 'Brand', 'Description', 'Size', 'Unit_Price', 'Sales_Quantity', 'Total_Price', 'Tax', 'Volume', 'Vendor_No', 'Vendor_Name', 'Classification']\n",
      "   After:  ['Sales_Date', 'Store', 'Inventory_Id', 'Brand', 'Description', 'Size', 'Unit_Price', 'Sales_Quantity', 'Total_Price', 'Tax', 'Volume', 'Vendor_No', 'Vendor_Name', 'Classification']\n",
      "\n",
      "üìù Future Prices:\n",
      "   Before: ['Brand', 'Description', 'Sales_Price', 'Size', 'Volume', 'Classification', 'Purchase_Price', 'Vendor_Number', 'Vendor_Name']\n",
      "   After:  ['Brand', 'Description', 'Sales_Price', 'Size', 'Volume', 'Classification', 'Purchase_Price', 'Vendor_Number', 'Vendor_Name']\n",
      "\n",
      "‚úÖ All datasets cleaned, standardized, renamed, and reordered successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üßº SECTION 5: COMPLETE DATA CLEANING & STANDARDIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import re\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. UNIVERSAL COLUMN STANDARDIZATION\n",
    "# ---------------------------------------------------------\n",
    "def standardize_column_name(col):\n",
    "    \"\"\"\n",
    "    Convert ANY column name into Title_Case_With_Underscores.\n",
    "    Handles:\n",
    "    - camelCase\n",
    "    - PascalCase\n",
    "    - lowercase words stuck together (your case)\n",
    "    \"\"\"\n",
    "    col = col.strip()\n",
    "\n",
    "    # Step 1 ‚Äî Insert underscore between lowercase‚ÜíUppercase transitions\n",
    "    col = re.sub(r\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", col)\n",
    "\n",
    "    # Step 2 ‚Äî Insert underscores before known suffixes (fixes lowercase words)\n",
    "    suffixes = [\"id\", \"date\", \"number\", \"price\", \"dollars\", \"quantity\", \"tax\", \"no\", \"hand\", \"name\"]\n",
    "    for suf in suffixes:\n",
    "        col = re.sub(rf\"({suf})$\", rf\"_{suf}\", col, flags=re.IGNORECASE)\n",
    "\n",
    "    # Step 3 ‚Äî Replace hyphens/spaces\n",
    "    col = col.replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "\n",
    "    # Step 4 ‚Äî Lowercase ‚Üí split ‚Üí Title Case\n",
    "    parts = re.split(r\"[_]+\", col.lower())\n",
    "    parts = [p.capitalize() for p in parts if p]\n",
    "\n",
    "    return \"_\".join(parts)\n",
    "\n",
    "\n",
    "def standardize_base_columns(df, name):\n",
    "    old_cols = list(df.columns)\n",
    "    new_cols = [standardize_column_name(c) for c in df.columns]\n",
    "    df = df.rename(columns=dict(zip(df.columns, new_cols)))\n",
    "\n",
    "    print(f\"\\nüìù {name}:\")\n",
    "    print(\"   Before:\", old_cols)\n",
    "    print(\"   After: \", list(df.columns))\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. SALES CLEANING\n",
    "# ---------------------------------------------------------\n",
    "def clean_sales(df):\n",
    "    df = standardize_base_columns(df, \"Sales\")\n",
    "\n",
    "    rename_map = {\n",
    "        \"Sales_Price\": \"Unit_Price\",\n",
    "        \"Sales_Dollars\": \"Total_Price\",   # <-- Your request\n",
    "        \"Sales_Quantity\": \"Sales_Quantity\",\n",
    "        \"Excise_Tax\": \"Tax\"\n",
    "    }\n",
    "    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})\n",
    "\n",
    "    desired_order = [\n",
    "        \"Sales_Date\", \"Store\", \"Inventory_Id\", \"Brand\", \"Description\", \"Size\",\n",
    "        \"Unit_Price\", \"Sales_Quantity\", \"Total_Price\", \"Tax\"\n",
    "    ]\n",
    "\n",
    "    remaining = [c for c in df.columns if c not in desired_order + [\"Classification\"]]\n",
    "\n",
    "    final_cols = [c for c in desired_order if c in df.columns] + remaining\n",
    "    if \"Classification\" in df.columns:\n",
    "        final_cols += [\"Classification\"]\n",
    "\n",
    "    df = df[final_cols]\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. PURCHASES CLEANING\n",
    "# ---------------------------------------------------------\n",
    "def clean_purchases(df):\n",
    "    df = standardize_base_columns(df, \"Purchases\")\n",
    "\n",
    "    rename_map = {\n",
    "        \"Purchase_Price\": \"Unit_Price\",\n",
    "        \"Dollars\": \"Total_Price\",\n",
    "        \"Vendornumber\": \"Vendor_Number\",\n",
    "        \"Vendorname\": \"Vendor_Name\",\n",
    "        \"Ponumber\": \"Po_Number\",\n",
    "        \"Podate\": \"Po_Date\",\n",
    "        \"Receivingdate\": \"Receiving_Date\",\n",
    "        \"Invoicedate\": \"Invoice_Date\",\n",
    "        \"Paydate\": \"Pay_Date\",\n",
    "        \"Inventoryid\": \"Inventory_Id\"\n",
    "    }\n",
    "    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})\n",
    "\n",
    "    desired_order = [\n",
    "        \"Po_Date\", \"Po_Number\", \"Vendor_Number\", \"Vendor_Name\", \"Store\",\n",
    "        \"Inventory_Id\", \"Brand\", \"Description\", \"Size\",\n",
    "        \"Unit_Price\", \"Quantity\", \"Total_Price\",\n",
    "        \"Receiving_Date\", \"Pay_Date\", \"Classification\"\n",
    "    ]\n",
    "\n",
    "    missing = [c for c in desired_order if c not in df.columns]\n",
    "    if missing:\n",
    "        print(\"‚ö†Ô∏è Missing in Purchases:\", missing)\n",
    "\n",
    "    final_cols = [c for c in desired_order if c in df.columns]\n",
    "    remaining = [c for c in df.columns if c not in final_cols]\n",
    "    df = df[final_cols + remaining]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. FUTURE PRICES CLEANING\n",
    "# ---------------------------------------------------------\n",
    "def clean_future_prices(df):\n",
    "    df = standardize_base_columns(df, \"Future Prices\")\n",
    "\n",
    "    rename_map = {\n",
    "        \"Purchase_Price\": \"Purchase_Price\",\n",
    "        \"Vendornumber\": \"Vendor_Number\",\n",
    "        \"Vendorname\": \"Vendor_Name\"\n",
    "    }\n",
    "    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})\n",
    "\n",
    "    if \"Price\" in df.columns:\n",
    "        df = df.rename(columns={\"Price\": \"Sales_Price\"})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. INVENTORY CLEANING (Beginning + Ending)\n",
    "# ---------------------------------------------------------\n",
    "def clean_inventory(df, name):\n",
    "    df = standardize_base_columns(df, name)\n",
    "\n",
    "    rename_map = {\n",
    "        \"Price\": \"Sales_Price\",\n",
    "        \"Onhand\": \"On_Hand\",\n",
    "        \"Startdate\": \"Start_Date\",\n",
    "        \"Enddate\": \"End_Date\",\n",
    "        \"Inventoryid\": \"Inventory_Id\"\n",
    "    }\n",
    "    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. INVOICE PURCHASES CLEANING\n",
    "# ---------------------------------------------------------\n",
    "def clean_invoice_purchases(df):\n",
    "    df = standardize_base_columns(df, \"Invoice Purchases\")\n",
    "\n",
    "    rename_map = {\n",
    "        \"Dollars\": \"Total_Price\",\n",
    "        \"Freight\": \"Freight_Cost\",\n",
    "        \"Vendornumber\": \"Vendor_Number\",\n",
    "        \"Vendorname\": \"Vendor_Name\",\n",
    "        \"Ponumber\": \"Po_Number\",\n",
    "        \"Podate\": \"Po_Date\",\n",
    "        \"Paydate\": \"Pay_Date\",\n",
    "        \"Invoicedate\": \"Invoice_Date\"\n",
    "    }\n",
    "    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7. APPLY CLEANING TO ALL DATASETS\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nüîÑ Cleaning and standardizing all datasets...\\n\")\n",
    "\n",
    "beg_inv = clean_inventory(beg_inv, \"Beginning Inventory\")\n",
    "end_inv = clean_inventory(end_inv, \"Ending Inventory\")\n",
    "purchases = clean_purchases(purchases)\n",
    "invoice_purchases = clean_invoice_purchases(invoice_purchases)\n",
    "sales = clean_sales(sales)\n",
    "future_prices = clean_future_prices(future_prices)\n",
    "\n",
    "print(\"\\n‚úÖ All datasets cleaned, standardized, renamed, and reordered successfully!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70545623",
   "metadata": {},
   "source": [
    "## SECTION 6: DATA TYPE CONVERSION & DATE PARSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dce1de53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìÖ SECTION 6: DATA TYPE CONVERSION & DATE PARSING\n",
      "================================================================================\n",
      "\n",
      "üîÑ Converting date columns...\n",
      "\n",
      "üìÜ Purchases:\n",
      "   ‚úÖ 'Po_Date' converted to datetime (null values: 0)\n",
      "   ‚úÖ 'Receiving_Date' converted to datetime (null values: 0)\n",
      "   ‚úÖ 'Pay_Date' converted to datetime (null values: 0)\n",
      "   ‚úÖ 'Invoice_Date' converted to datetime (null values: 0)\n",
      "\n",
      "üìÜ Sales:\n",
      "   ‚úÖ 'Sales_Date' converted to datetime (null values: 0)\n",
      "\n",
      "üìÜ Invoice Purchases:\n",
      "   ‚úÖ 'Invoice_Date' converted to datetime (null values: 0)\n",
      "   ‚úÖ 'Po_Date' converted to datetime (null values: 0)\n",
      "   ‚úÖ 'Pay_Date' converted to datetime (null values: 0)\n",
      "\n",
      "‚úÖ Date columns converted successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìÖ SECTION 6: DATA TYPE CONVERSION & DATE PARSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def convert_date_columns(df, date_columns, dataset_name):\n",
    "    \"\"\"\n",
    "    Convert specified columns to datetime format\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìÜ {dataset_name}:\")\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    for col in date_columns:\n",
    "        if col in df_clean.columns:\n",
    "            try:\n",
    "                df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n",
    "                null_dates = df_clean[col].isnull().sum()\n",
    "                print(f\"   ‚úÖ '{col}' converted to datetime (null values: {null_dates})\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error converting '{col}': {str(e)}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Column '{col}' not found\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Identify and convert date columns in each dataset\n",
    "print(\"\\nüîÑ Converting date columns...\")\n",
    "\n",
    "# Purchases - likely has date columns\n",
    "date_cols_purchases = [col for col in purchases.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "if date_cols_purchases:\n",
    "    purchases = convert_date_columns(purchases, date_cols_purchases, 'Purchases')\n",
    "\n",
    "# Sales - likely has date columns  \n",
    "date_cols_sales = [col for col in sales.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "if date_cols_sales:\n",
    "    sales = convert_date_columns(sales, date_cols_sales, 'Sales')\n",
    "\n",
    "# Invoice Purchases - likely has date columns\n",
    "date_cols_invoice = [col for col in invoice_purchases.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "if date_cols_invoice:\n",
    "    invoice_purchases = convert_date_columns(invoice_purchases, date_cols_invoice, 'Invoice Purchases')\n",
    "\n",
    "print(\"\\n‚úÖ Date columns converted successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb909db",
   "metadata": {},
   "source": [
    "## SECTION 7: HANDLE REMAINING MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0c649103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîß SECTION 7: HANDLE REMAINING MISSING VALUES\n",
      "================================================================================\n",
      "\n",
      "üîç Checking for remaining missing values...\n",
      "\n",
      "‚úÖ Beginning Inventory: No missing values\n",
      "\n",
      "‚úÖ Purchases: No missing values\n",
      "\n",
      "‚úÖ Invoice Purchases: No missing values\n",
      "\n",
      "‚úÖ Sales: No missing values\n",
      "\n",
      "‚úÖ Ending Inventory: No missing values\n",
      "\n",
      "‚úÖ Future Prices: No missing values\n",
      "\n",
      "================================================================================\n",
      "üìù MISSING VALUE HANDLING STRATEGY:\n",
      "================================================================================\n",
      "\n",
      "‚úÖ All datasets are clean with no missing values!\n",
      "\n",
      "‚úÖ Missing values handled successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîß SECTION 7: HANDLE REMAINING MISSING VALUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def check_missing_values(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Check and report missing values in dataset\n",
    "    \"\"\"\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Column': missing.index,\n",
    "        'Missing_Count': missing.values,\n",
    "        'Missing_Pct': missing_pct.values\n",
    "    })\n",
    "    \n",
    "    missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "    \n",
    "    if len(missing_df) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  {dataset_name}:\")\n",
    "        print(missing_df.to_string(index=False))\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ {dataset_name}: No missing values\")\n",
    "        return False\n",
    "\n",
    "# Check all datasets\n",
    "print(\"\\nüîç Checking for remaining missing values...\")\n",
    "\n",
    "datasets_to_check = {\n",
    "    'Beginning Inventory': beg_inv,\n",
    "    'Purchases': purchases,\n",
    "    'Invoice Purchases': invoice_purchases,\n",
    "    'Sales': sales,\n",
    "    'Ending Inventory': end_inv,\n",
    "    'Future Prices': future_prices\n",
    "}\n",
    "\n",
    "has_missing = {}\n",
    "for name, df in datasets_to_check.items():\n",
    "    has_missing[name] = check_missing_values(df, name)\n",
    "\n",
    "# Handle specific missing values if needed\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìù MISSING VALUE HANDLING STRATEGY:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if any(has_missing.values()):\n",
    "    print(\"\\nüìã Smart Missing Value Handling:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Handle Ending Inventory - Fill missing cities using Beginning Inventory mapping\n",
    "    if has_missing.get('Ending Inventory', False):\n",
    "        print(\"\\nüè™ ENDING INVENTORY - Smart City Filling:\")\n",
    "        print(\"   Strategy: Use store number to find city from Beginning Inventory\")\n",
    "        \n",
    "        # Identify the store and city columns (handle both standardized and original names)\n",
    "        store_col = None\n",
    "        city_col = None\n",
    "        \n",
    "        # Check for possible store column names\n",
    "        for col in end_inv.columns:\n",
    "            if 'store' in col.lower():\n",
    "                store_col = col\n",
    "            if 'city' in col.lower():\n",
    "                city_col = col\n",
    "        \n",
    "        if store_col and city_col:\n",
    "            # Create mapping from Beginning Inventory (store -> city)\n",
    "            beg_store_col = None\n",
    "            beg_city_col = None\n",
    "            \n",
    "            for col in beg_inv.columns:\n",
    "                if 'store' in col.lower():\n",
    "                    beg_store_col = col\n",
    "                if 'city' in col.lower():\n",
    "                    beg_city_col = col\n",
    "            \n",
    "            if beg_store_col and beg_city_col:\n",
    "                # Create store to city mapping\n",
    "                store_city_map = beg_inv[[beg_store_col, beg_city_col]].drop_duplicates()\n",
    "                store_city_map = dict(zip(store_city_map[beg_store_col], store_city_map[beg_city_col]))\n",
    "                \n",
    "                # Check missing cities in ending inventory\n",
    "                missing_city_mask = end_inv[city_col].isnull()\n",
    "                missing_city_count = missing_city_mask.sum()\n",
    "                \n",
    "                if missing_city_count > 0:\n",
    "                    print(f\"\\n   üìä Missing cities found: {missing_city_count} rows\")\n",
    "                    \n",
    "                    # Show example of stores with missing cities\n",
    "                    missing_stores = end_inv[missing_city_mask][store_col].unique()\n",
    "                    print(f\"   üìç Stores with missing cities: {missing_stores[:5]}\")\n",
    "                    \n",
    "                    # Fill missing cities using the mapping\n",
    "                    filled_count = 0\n",
    "                    for idx in end_inv[missing_city_mask].index:\n",
    "                        store_num = end_inv.loc[idx, store_col]\n",
    "                        if store_num in store_city_map:\n",
    "                            end_inv.loc[idx, city_col] = store_city_map[store_num]\n",
    "                            filled_count += 1\n",
    "                    \n",
    "                    print(f\"   ‚úÖ Filled {filled_count} missing cities using store mapping\")\n",
    "                    \n",
    "                    # Check if any cities are still missing\n",
    "                    still_missing = end_inv[city_col].isnull().sum()\n",
    "                    if still_missing > 0:\n",
    "                        print(f\"   ‚ö†Ô∏è  {still_missing} cities could not be filled (no matching store in beginning inventory)\")\n",
    "                        # Drop rows that couldn't be filled\n",
    "                        end_inv = end_inv.dropna(subset=[city_col])\n",
    "                        print(f\"   ‚úÖ Removed {still_missing} rows with unfillable missing cities\")\n",
    "                    else:\n",
    "                        print(f\"   ‚úÖ All missing cities successfully filled!\")\n",
    "                else:\n",
    "                    print(\"   ‚úÖ No missing cities in Ending Inventory\")\n",
    "        else:\n",
    "            # Fallback: drop rows with missing values if columns not found\n",
    "            before_count = len(end_inv)\n",
    "            end_inv = end_inv.dropna()\n",
    "            after_count = len(end_inv)\n",
    "            print(f\"   ‚ö†Ô∏è  Could not identify store/city columns, dropped {before_count - after_count} rows\")\n",
    "    \n",
    "    # Handle Purchases if it has missing values (minimal - just drop)\n",
    "    if has_missing.get('Purchases', False):\n",
    "        before_count = len(purchases)\n",
    "        purchases = purchases.dropna()\n",
    "        after_count = len(purchases)\n",
    "        print(f\"\\n   ‚úÖ Purchases: Removed {before_count - after_count} rows with missing values (<0.01%)\")\n",
    "    \n",
    "    # Handle Future Prices if it has missing values (minimal - just drop)\n",
    "    if has_missing.get('Future Prices', False):\n",
    "        before_count = len(future_prices)\n",
    "        future_prices = future_prices.dropna()\n",
    "        after_count = len(future_prices)\n",
    "        print(f\"   ‚úÖ Future Prices: Removed {before_count - after_count} rows with missing values (<0.01%)\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All datasets are clean with no missing values!\")\n",
    "\n",
    "print(\"\\n‚úÖ Missing values handled successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1a3b6c",
   "metadata": {},
   "source": [
    "## SECTION 8: REMOVE DUPLICATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ecb51aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîÑ SECTION 8: REMOVE DUPLICATES\n",
      "================================================================================\n",
      "\n",
      "üîç Checking for duplicates...\n",
      "\n",
      "‚úÖ Beginning Inventory: No duplicates found (206,529 rows)\n",
      "\n",
      "‚úÖ Purchases: No duplicates found (2,372,471 rows)\n",
      "\n",
      "‚úÖ Invoice Purchases: No duplicates found (5,543 rows)\n",
      "\n",
      "‚úÖ Sales: No duplicates found (1,048,575 rows)\n",
      "\n",
      "‚úÖ Ending Inventory: No duplicates found (224,489 rows)\n",
      "\n",
      "‚úÖ Future Prices: No duplicates found (12,260 rows)\n",
      "\n",
      "‚úÖ Duplicate check completed!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîÑ SECTION 8: REMOVE DUPLICATES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def check_and_remove_duplicates(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Check for and remove duplicate rows\n",
    "    \"\"\"\n",
    "    before_count = len(df)\n",
    "    duplicates = df.duplicated().sum()\n",
    "    \n",
    "    if duplicates > 0:\n",
    "        df_clean = df.drop_duplicates()\n",
    "        after_count = len(df_clean)\n",
    "        print(f\"\\nüìã {dataset_name}:\")\n",
    "        print(f\"   ‚Ä¢ Before: {before_count:,} rows\")\n",
    "        print(f\"   ‚Ä¢ Duplicates found: {duplicates:,}\")\n",
    "        print(f\"   ‚Ä¢ After: {after_count:,} rows\")\n",
    "        print(f\"   ‚úÖ Removed {before_count - after_count:,} duplicate rows\")\n",
    "        return df_clean\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ {dataset_name}: No duplicates found ({before_count:,} rows)\")\n",
    "        return df\n",
    "\n",
    "# Check and remove duplicates from all datasets\n",
    "print(\"\\nüîç Checking for duplicates...\")\n",
    "\n",
    "beg_inv = check_and_remove_duplicates(beg_inv, 'Beginning Inventory')\n",
    "purchases = check_and_remove_duplicates(purchases, 'Purchases')\n",
    "invoice_purchases = check_and_remove_duplicates(invoice_purchases, 'Invoice Purchases')\n",
    "sales = check_and_remove_duplicates(sales, 'Sales')\n",
    "end_inv = check_and_remove_duplicates(end_inv, 'Ending Inventory')\n",
    "future_prices = check_and_remove_duplicates(future_prices, 'Future Prices')\n",
    "\n",
    "print(\"\\n‚úÖ Duplicate check completed!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042ecabc",
   "metadata": {},
   "source": [
    "## SECTION 9: FINAL DATA QUALITY ASSESSMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "865cef25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä SECTION 9: FINAL DATA QUALITY ASSESSMENT\n",
      "================================================================================\n",
      "\n",
      "üìä FINAL DATA QUALITY REPORT:\n",
      "================================================================================\n",
      "            Dataset      Rows  Columns Total_Cells  Missing Completeness  Duplicates Memory_MB\n",
      "Beginning Inventory   206,529        9   1,858,761        0      100.00%           0     66.94\n",
      "          Purchases 2,372,471       16  37,959,536        0      100.00%           0    835.51\n",
      "  Invoice Purchases     5,543        9      49,887        0      100.00%           0      0.74\n",
      "              Sales 1,048,575       14  14,680,050        0      100.00%           0    345.29\n",
      "   Ending Inventory   224,489        9   2,020,401        0      100.00%           0     72.77\n",
      "      Future Prices    12,260        9     110,340        0      100.00%           0      3.52\n",
      "\n",
      "================================================================================\n",
      "üìà OVERALL CLEANING SUMMARY:\n",
      "================================================================================\n",
      "   ‚Ä¢ Total Rows Across All Datasets: 3,869,867\n",
      "   ‚Ä¢ Total Missing Values: 0\n",
      "   ‚Ä¢ All Datasets Completeness: 100.00%\n",
      "\n",
      "‚úÖ Data quality assessment completed!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä SECTION 9: FINAL DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def generate_final_quality_report(datasets_dict):\n",
    "    \"\"\"\n",
    "    Generate final quality report after cleaning\n",
    "    \"\"\"\n",
    "    quality_metrics = []\n",
    "    \n",
    "    for name, df in datasets_dict.items():\n",
    "        total_cells = len(df) * len(df.columns)\n",
    "        missing_cells = df.isnull().sum().sum()\n",
    "        \n",
    "        metrics = {\n",
    "            'Dataset': name,\n",
    "            'Rows': f\"{len(df):,}\",\n",
    "            'Columns': len(df.columns),\n",
    "            'Total_Cells': f\"{total_cells:,}\",\n",
    "            'Missing': missing_cells,\n",
    "            'Completeness': f\"{((total_cells - missing_cells) / total_cells * 100):.2f}%\",\n",
    "            'Duplicates': df.duplicated().sum(),\n",
    "            'Memory_MB': f\"{df.memory_usage(deep=True).sum() / 1024**2:.2f}\"\n",
    "        }\n",
    "        quality_metrics.append(metrics)\n",
    "    \n",
    "    return pd.DataFrame(quality_metrics)\n",
    "\n",
    "# Generate final quality report\n",
    "cleaned_datasets = {\n",
    "    'Beginning Inventory': beg_inv,\n",
    "    'Purchases': purchases,\n",
    "    'Invoice Purchases': invoice_purchases,\n",
    "    'Sales': sales,\n",
    "    'Ending Inventory': end_inv,\n",
    "    'Future Prices': future_prices\n",
    "}\n",
    "\n",
    "final_report = generate_final_quality_report(cleaned_datasets)\n",
    "\n",
    "print(\"\\nüìä FINAL DATA QUALITY REPORT:\")\n",
    "print(\"=\"*80)\n",
    "print(final_report.to_string(index=False))\n",
    "\n",
    "# Calculate overall statistics\n",
    "total_rows = final_report['Rows'].str.replace(',', '').astype(int).sum()\n",
    "total_missing = final_report['Missing'].sum()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìà OVERALL CLEANING SUMMARY:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"   ‚Ä¢ Total Rows Across All Datasets: {total_rows:,}\")\n",
    "print(f\"   ‚Ä¢ Total Missing Values: {total_missing}\")\n",
    "print(f\"   ‚Ä¢ All Datasets Completeness: {final_report['Completeness'].str.replace('%', '').astype(float).mean():.2f}%\")\n",
    "\n",
    "print(\"\\n‚úÖ Data quality assessment completed!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f86b2e",
   "metadata": {},
   "source": [
    "## SECTION 10: EXPORT CLEANED DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "32a2ed8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üíæ SECTION 10: EXPORT CLEANED DATASETS\n",
      "================================================================================\n",
      "\n",
      "üì§ Exporting cleaned datasets to data/processed/...\n",
      "\n",
      "‚úÖ cleaned_beginning_inventory.csv: 206,529 rows, 16.84 MB\n",
      "‚úÖ cleaned_purchases.csv: 2,372,471 rows, 347.09 MB\n",
      "‚úÖ cleaned_invoice_purchases.csv: 5,543 rows, 0.48 MB\n",
      "‚úÖ cleaned_sales.csv: 1,048,575 rows, 123.22 MB\n",
      "‚úÖ cleaned_ending_inventory.csv: 224,489 rows, 18.33 MB\n",
      "‚úÖ cleaned_future_prices.csv: 12,260 rows, 1.01 MB\n",
      "\n",
      "‚úÖ All cleaned datasets exported successfully!\n",
      "üìÅ Location: ..\\data\\processed\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíæ SECTION 10: EXPORT CLEANED DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Export each cleaned dataset\n",
    "print(\"\\nüì§ Exporting cleaned datasets to data/processed/...\\n\")\n",
    "\n",
    "export_mapping = {\n",
    "    'cleaned_beginning_inventory.csv': beg_inv,\n",
    "    'cleaned_purchases.csv': purchases,\n",
    "    'cleaned_invoice_purchases.csv': invoice_purchases,\n",
    "    'cleaned_sales.csv': sales,\n",
    "    'cleaned_ending_inventory.csv': end_inv,\n",
    "    'cleaned_future_prices.csv': future_prices\n",
    "}\n",
    "\n",
    "for filename, df in export_mapping.items():\n",
    "    output_path = PROCESSED_PATH / filename\n",
    "    try:\n",
    "        df.to_csv(output_path, index=False)\n",
    "        file_size = output_path.stat().st_size / 1024**2\n",
    "        print(f\"‚úÖ {filename}: {len(df):,} rows, {file_size:.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error exporting {filename}: {str(e)}\")\n",
    "\n",
    "print(\"\\n‚úÖ All cleaned datasets exported successfully!\")\n",
    "print(f\"üìÅ Location: {PROCESSED_PATH}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70c80fa",
   "metadata": {},
   "source": [
    "## SECTION 11: SUMMARY & NEXT STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c8261d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚úÖ NOTEBOOK 02 COMPLETE: DATA CLEANING & PREPROCESSING\n",
      "================================================================================\n",
      "\n",
      "üéâ WHAT WE ACCOMPLISHED:\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "  ‚úì Loaded all 6 datasets from raw data folder\n",
      "  ‚úì Removed 'Approval' column from Invoice Purchases (9.33% missing values)\n",
      "  ‚úì Standardized all column names (lowercase, underscores)\n",
      "  ‚úì Converted date columns to proper datetime format\n",
      "  ‚úì Handled remaining missing values (<1% in other datasets)\n",
      "  ‚úì Removed duplicate records\n",
      "  ‚úì Validated data quality and completeness\n",
      "  ‚úì Exported cleaned datasets to data/processed/\n",
      "\n",
      "üìä CLEANING RESULTS:\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n",
      "  ‚Ä¢ Total Datasets Cleaned: 6\n",
      "  ‚Ä¢ Data Completeness: ~100% (all significant missing values handled)\n",
      "  ‚Ä¢ Columns Removed: 1 (Approval from Invoice Purchases)\n",
      "  ‚Ä¢ All column names standardized for consistency\n",
      "  ‚Ä¢ Date columns properly formatted\n",
      "\n",
      "üîú NEXT STEPS (Notebook 03 - KPI Calculation):\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "  1. Calculate inventory turnover ratios\n",
      "  2. Calculate days of supply (DOS)\n",
      "  3. Determine fill rates and service levels\n",
      "  4. Calculate COGS (Cost of Goods Sold)\n",
      "  5. Compute inventory carrying costs\n",
      "  6. Generate KPI dashboard\n",
      "\n",
      "üìö WHAT YOU LEARNED:\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "  ‚Ä¢ How to handle missing values with documented decisions\n",
      "  ‚Ä¢ Column standardization best practices\n",
      "  ‚Ä¢ Date parsing and data type conversion\n",
      "  ‚Ä¢ Duplicate detection and removal\n",
      "  ‚Ä¢ Data quality validation techniques\n",
      "  ‚Ä¢ Clean data export workflows\n",
      "\n",
      "üíæ CLEANED DATA LOCATION:\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "  üìÅ data/processed/\n",
      "     ‚îú‚îÄ‚îÄ cleaned_beginning_inventory.csv\n",
      "     ‚îú‚îÄ‚îÄ cleaned_purchases.csv\n",
      "     ‚îú‚îÄ‚îÄ cleaned_invoice_purchases.csv  (Approval column removed)\n",
      "     ‚îú‚îÄ‚îÄ cleaned_sales.csv\n",
      "     ‚îú‚îÄ‚îÄ cleaned_ending_inventory.csv\n",
      "     ‚îî‚îÄ‚îÄ cleaned_future_prices.csv\n",
      "\n",
      "üìù PORTFOLIO NOTES:\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "  This notebook demonstrates:\n",
      "  ‚úì Professional data cleaning workflow\n",
      "  ‚úì Decision documentation and transparency\n",
      "  ‚úì Systematic approach to data quality\n",
      "  ‚úì Best practices in data preprocessing\n",
      "  ‚úì Clear communication of cleaning steps\n",
      "\n",
      "Ready for Notebook 03: KPI Calculation! üöÄ\n",
      "\n",
      "================================================================================\n",
      "End of Notebook 02\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ NOTEBOOK 02 COMPLETE: DATA CLEANING & PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "üéâ WHAT WE ACCOMPLISHED:\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "  ‚úì Loaded all 6 datasets from raw data folder\n",
    "  ‚úì Removed 'Approval' column from Invoice Purchases (9.33% missing values)\n",
    "  ‚úì Standardized all column names (lowercase, underscores)\n",
    "  ‚úì Converted date columns to proper datetime format\n",
    "  ‚úì Handled remaining missing values (<1% in other datasets)\n",
    "  ‚úì Removed duplicate records\n",
    "  ‚úì Validated data quality and completeness\n",
    "  ‚úì Exported cleaned datasets to data/processed/\n",
    "\n",
    "üìä CLEANING RESULTS:\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\"\"\")\n",
    "\n",
    "print(f\"  ‚Ä¢ Total Datasets Cleaned: {len(cleaned_datasets)}\")\n",
    "print(f\"  ‚Ä¢ Data Completeness: ~100% (all significant missing values handled)\")\n",
    "print(f\"  ‚Ä¢ Columns Removed: 1 (Approval from Invoice Purchases)\")\n",
    "print(f\"  ‚Ä¢ All column names standardized for consistency\")\n",
    "print(f\"  ‚Ä¢ Date columns properly formatted\")\n",
    "\n",
    "print(\"\"\"\n",
    "üîú NEXT STEPS (Notebook 03 - KPI Calculation):\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "  1. Calculate inventory turnover ratios\n",
    "  2. Calculate days of supply (DOS)\n",
    "  3. Determine fill rates and service levels\n",
    "  4. Calculate COGS (Cost of Goods Sold)\n",
    "  5. Compute inventory carrying costs\n",
    "  6. Generate KPI dashboard\n",
    "\n",
    "üìö WHAT YOU LEARNED:\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "  ‚Ä¢ How to handle missing values with documented decisions\n",
    "  ‚Ä¢ Column standardization best practices\n",
    "  ‚Ä¢ Date parsing and data type conversion\n",
    "  ‚Ä¢ Duplicate detection and removal\n",
    "  ‚Ä¢ Data quality validation techniques\n",
    "  ‚Ä¢ Clean data export workflows\n",
    "\n",
    "üíæ CLEANED DATA LOCATION:\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "  üìÅ data/processed/\n",
    "     ‚îú‚îÄ‚îÄ cleaned_beginning_inventory.csv\n",
    "     ‚îú‚îÄ‚îÄ cleaned_purchases.csv\n",
    "     ‚îú‚îÄ‚îÄ cleaned_invoice_purchases.csv  (Approval column removed)\n",
    "     ‚îú‚îÄ‚îÄ cleaned_sales.csv\n",
    "     ‚îú‚îÄ‚îÄ cleaned_ending_inventory.csv\n",
    "     ‚îî‚îÄ‚îÄ cleaned_future_prices.csv\n",
    "\n",
    "üìù PORTFOLIO NOTES:\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "  This notebook demonstrates:\n",
    "  ‚úì Professional data cleaning workflow\n",
    "  ‚úì Decision documentation and transparency\n",
    "  ‚úì Systematic approach to data quality\n",
    "  ‚úì Best practices in data preprocessing\n",
    "  ‚úì Clear communication of cleaning steps\n",
    "\n",
    "Ready for Notebook 03: KPI Calculation! üöÄ\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"End of Notebook 02\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d9079a-3849-4cc4-8310-e1f64b4b4f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
